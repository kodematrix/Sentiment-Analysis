---
title: "Text Analysis"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

##Bhagavad Gita
The Bhagavad Gita, the “Song of the Divine One”, is a Sanskrit text from the Mahabharata. The Gita consists of 700 versus, and is spoken mainly by Krishna, who is explaining the purpose of life to Arjuna.

##Text Analysis

The Goal of this project is to analyze the text of the holy book and we will try to look for some meaningful insights.

There are many packages or libraries that helps us decode the text in to vectors. out of which, we are going to use the following libraries.
```{r}
library(pdftools)
library(tidytext)
library(tm)
library(tidyverse)
library(ggplot2)
library(tidytext)
library(scales)
```

Choosing and importing the file to RStudio.
```{r}
Bhagavad_gita <- choose.files()
Text_1 <- pdf_text(Bhagavad_gita)
```

#Splitting the text and storing it to tibble form

There are several functions we used from various libraries,

unlist() - used to convert string to vectors
gsub()- this function used to replace all matches of an existing text with the give string argument
tibble() - Tibbles are a modern take on data frames. They keep the features that have stood the test of time, and drop the features that used to be convenient but are now frustrating (i.e. converting character vectors to factors).  
```{r}
Text_2 <- strsplit(Text_1,"\n")
Text_unlisted <- unlist(Text_2) 
Text_unlisted <- gsub("\r","",Text_unlisted,ignore.case = T)   
Text_unlisted <- gsub("Kåñëa","Krishnaa",Text_unlisted)
Text_unlisted<- gsub("Bhagavad-gétä","Bhagavad-Gita",Text_unlisted)
Text_unlisted<- gsub("Gétä","Gita",Text_unlisted)
tibble_form <- tibble(line=1:length(Text_unlisted),text=Text_unlisted) 
```


#Converting text to tokens and removing lexions and other noises
```{r}
text_df <- tibble_form %>% unnest_tokens(word,text)
data("stop_words")
lexicon <- stop_words %>% filter(lexicon=="SMART")
tidy_text <- text_df %>% anti_join(lexicon)
extra_words <- c("copyright","1998","bhaktivedanta","int'l","reserved","rights","book","trust","ca","translation","purport","na","synonyms","text")
extra_words <- tibble(extra_words)
colnames(extra_words) <- c("word")
tidy_text <- tidy_text %>%  anti_join(extra_words,by="word")
```

#Filtering words greater than 500 times
```{r}
tidy_text  %>% count(word,sort = T) %>% filter(n>500) %>% mutate(word=reorder(word,n)) %>% 
  ggplot(aes(word,n)) + geom_col() + coord_flip() + 
    labs(x="Count",title = "Top Words")+
    theme(plot.title = element_text(hjust = 0.5))
```

# extracting sentiments 
```{r}
sentiments <- tidy_text %>% inner_join(get_sentiments("bing")) %>% mutate(linenumber= row_number()) %>%
  mutate(index= linenumber %/% 80 ) %>% 
   count(index,sentiment) %>% 
    spread(sentiment,n,fill=0) %>%
      mutate(weight=positive-negative)
```

# visulas of weighatge of positive - negative words per 80 words
```{r}
effect <- ifelse(sentiments$weight<0,"Negative","Positive")
sentiments %>% ggplot(aes(index,weight,fill=effect)) + geom_col()
```

# extracting and plotting top ten words in both negative and positive category
```{r}
top_ten <- tidy_text %>% inner_join(get_sentiments("bing")) %>% count(word,sentiment,sort=T) %>% ungroup()

top_ten %>% group_by(sentiment) %>% top_n(10) %>% ungroup() %>% mutate(word=reorder(word,n)) %>%
  ggplot(aes(word,n,fill=sentiment)) + geom_col(show.legend = TRUE) + 
    facet_wrap(~sentiment,scale="free_y")+
      labs(y="Contribution to Sentiment",x=NULL,title = "Top 10 Words") +
      coord_flip() +
        theme(plot.title = element_text(hjust = 0.5))
```

# extracting important words with respect to each chapter
```{r}
change_over_time <- tibble_form %>% 
   mutate(chapter=cumsum(str_detect(text,"CHAPTER"))) %>% 
     unnest_tokens(word,text) %>% anti_join(lexicon) %>%
        filter(word %in% c("krishnaa","ignorance","illusion","spiritual","arjuna","lord"))
          
# calculating proportion of selected words in each chapter
ch <- change_over_time %>%  count(chapter,word) %>% group_by(chapter) %>%
  mutate(total=sum(n)) %>% ungroup() %>% mutate(proportion=n/total)
```

# plotting chnage in usage of selected words over time chapters.
```{r}

ch %>% ggplot(aes(chapter,proportion)) + geom_point() + geom_smooth()+
  facet_wrap(~word,scale="free_y") +
      labs(y="% of frequency of words over chapters",title="Change in all 18 chapters") +
        theme(plot.title = element_text(hjust = 0.5))

```

